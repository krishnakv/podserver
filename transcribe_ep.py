import json
import logging
import sys
import time
from enum import Enum

import psycopg2
import requests
import swagger_client
from azure.cognitiveservices.speech import AudioConfig, SpeechConfig, SpeechRecognizer
from openai import AzureOpenAI
from psycopg2 import sql


class EpisodeAttributes(Enum):
    ID = 0
    PODCASTID = 1
    EPISODEID = 2
    TITLE = 3
    SUMMARY = 4
    URL = 5
    AUTHORS = 6
    PUBLISHED = 7
    DURATION = 8
    QUESTIONS = 9
    TRANSCRIBED = 10
    TRANSCRIPT = 11
    TRANSCRIPTTEXT = 12


def select_from_episodes_with_episodeid(episodeid):
    # Read configuration from JSON file
    with open("config.json") as config_file:
        config = json.load(config_file)

    connection_string = f"dbname='{config["DBNAME"]}' user='{config["USER"]}' host='{config["HOST"]}' port='{config["PORT"]}' password='{config["PASS"]}'"

    # Connect to the PostgreSQL database
    conn = psycopg2.connect(connection_string)
    cursor = conn.cursor()

    # Prepare and execute an SQL SELECT statement
    select_query = sql.SQL("SELECT * FROM episodes WHERE episodeid = %s;").format(
        sql.Identifier("episodes")
    )

    cursor.execute(select_query, (episodeid,))

    # Fetch and print the result of the SELECT statement
    result = cursor.fetchall()
    print(result)

    # Close the cursor and the connection
    cursor.close()
    conn.close()

    return result[0]


logging.basicConfig(
    stream=sys.stdout,
    level=logging.DEBUG,
    format="%(asctime)s %(message)s",
    datefmt="%m/%d/%Y %I:%M:%S %p %Z",
)


def transcribe_from_single_file(uri, properties):
    """
    Transcribe a single audio file located at `uri` using the settings specified in `properties`
    using the base model for the specified locale.
    """
    transcription_definition = swagger_client.Transcription(
        display_name=NAME,
        description=DESCRIPTION,
        locale=LOCALE,
        content_urls=[uri],
        properties=properties,
    )

    print(transcription_definition)

    return transcription_definition


QUESTIONS_SYSTEM_PROMPT = """
You are a quizmaster who is an expert at crafting deep and meaningful questions 
on any topic. The questions you create will be insightful and demonstrate good 
knowledge of the topic and accompanying text on the part of the quiz participant 
if answered correctly. You have a precise and accurate style of writing with no 
ambiguity in the questions created.
"""

QUESTIONS_USER_PROMPT = """
The input enclosed in backticks is the transcript of a podcast with the title "%s". 
Based on this transcript, please create 10 questions that can be used as part of a 
quiz for this podcast episode. Please do this by first crafting 20 good questions 
and then shortlisting to the 10 best questions. Just return the 10 questions as a 
JSON array without any additional text. For example ["Question1", "Question2"]. 
Ignore any parts of the podcast that are not relevant to the core topic of the episode 
such as ad reads.
`%s`
"""


def create_questions_list(config, title, transcript_text):
    client = AzureOpenAI(
        api_key=config["LLM_API_KEY"],
        api_version=config["LLM_API_VERSION"],
        azure_endpoint=config["LLM_TARGET_URI"],
    )
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": QUESTIONS_SYSTEM_PROMPT},
            {
                "role": "user",
                "content": QUESTIONS_USER_PROMPT % (title, transcript_text),
            },
        ],
        response_format={"type": "json_object"},
        stream=False,
    )

    return response


def _paginate(api, paginated_object):
    """
    The autogenerated client does not support pagination. This function returns a generator over
    all items of the array that the paginated object `paginated_object` is part of.
    """
    yield from paginated_object.values
    typename = type(paginated_object).__name__
    auth_settings = ["api_key"]
    while paginated_object.next_link:
        link = paginated_object.next_link[len(api.api_client.configuration.host) :]
        paginated_object, status, headers = api.api_client.call_api(
            link, "GET", response_type=typename, auth_settings=auth_settings
        )

        if status == 200:
            yield from paginated_object.values
        else:
            raise Exception(f"could not receive paginated data: status {status}")


NAME = "Podcast transcription"
DESCRIPTION = "Moltocasto podcast app transcription"

LOCALE = "en-US"

# Provide the uri of a container with audio files for transcribing all of them
# with a single request. At least 'read' and 'list' (rl) permissions are required.
# RECORDINGS_CONTAINER_URI = "<Your SAS Uri to a container of audio files>"

# Set model information when doing transcription with custom models
MODEL_REFERENCE = None  # guid of a custom model


def transcribe(podcastid, episodeid, title, audio_url):
    logging.info("Starting transcription client...")

    # Read configuration from JSON file
    with open("config.json") as config_file:
        config = json.load(config_file)

    connection_string = f"dbname='{config["DBNAME"]}' user='{config["USER"]}' host='{config["HOST"]}' port='{config["PORT"]}' password='{config["PASS"]}'"

    # Prepare and execute an SQL UPDATE statement
    update_query = """
    UPDATE episodes
    SET questions = %s, transcribed = TRUE, transcript = %s, transcripttext = %s
    WHERE podcastid = %s AND episodeid = %s;
    """

    # Set up the Azure Speech configuration
    SUBSCRIPTION_KEY = config["SPEECH_KEY"]
    SERVICE_REGION = config["SERVICE_REGION"]

    # configure API key authorization: subscription_key
    configuration = swagger_client.Configuration()
    configuration.api_key["Ocp-Apim-Subscription-Key"] = SUBSCRIPTION_KEY
    configuration.host = (
        f"https://{SERVICE_REGION}.api.cognitive.microsoft.com/speechtotext/v3.2"
    )

    # create the client object and authenticate
    client = swagger_client.ApiClient(configuration)

    # create an instance of the transcription api class
    api = swagger_client.CustomSpeechTranscriptionsApi(api_client=client)

    # Specify transcription properties by passing a dict to the properties parameter. See
    # https://learn.microsoft.com/azure/cognitive-services/speech-service/batch-transcription-create?pivots=rest-api#request-configuration-options
    # for supported parameters.
    properties = swagger_client.TranscriptionProperties()
    # properties.word_level_timestamps_enabled = True
    # properties.display_form_word_level_timestamps_enabled = True
    # properties.punctuation_mode = "DictatedAndAutomatic"
    # properties.profanity_filter_mode = "Masked"
    # properties.destination_container_url = "<SAS Uri with at least write (w) permissions for an Azure Storage blob container that results should be written to>"
    # properties.time_to_live = "PT1H"

    # uncomment the following block to enable and configure speaker separation
    # properties.diarization_enabled = True
    # properties.diarization = swagger_client.DiarizationProperties(
    #     swagger_client.DiarizationSpeakersProperties(min_count=1, max_count=5))

    # uncomment the following block to enable and configure language identification prior to transcription. Available modes are "single" and "continuous".
    # properties.language_identification = swagger_client.LanguageIdentificationProperties(mode="single", candidate_locales=["en-US", "ja-JP"])

    # Use base models for transcription. Comment this block if you are using a custom model.
    transcription_definition = transcribe_from_single_file(audio_url, properties)

    # Uncomment this block to use custom models for transcription.
    # transcription_definition = transcribe_with_custom_model(client, RECORDINGS_BLOB_URI, properties)

    # uncomment the following block to enable and configure language identification prior to transcription
    # Uncomment this block to transcribe all files from a container.
    # transcription_definition = transcribe_from_container(RECORDINGS_CONTAINER_URI, properties)

    created_transcription, status, headers = api.transcriptions_create_with_http_info(
        transcription=transcription_definition
    )

    # get the transcription Id from the location URI
    transcription_id = headers["location"].split("/")[-1]

    # Log information about the created transcription. If you should ask for support, please
    # include this information.
    logging.info(
        f"Created new transcription with id '{transcription_id}' in region {SERVICE_REGION}"
    )

    logging.info("Checking status.")

    completed = False

    while not completed:
        # wait for 5 seconds before refreshing the transcription status
        time.sleep(20)

        transcription = api.transcriptions_get(transcription_id)
        logging.info(f"Transcriptions status: {transcription.status}")

        if transcription.status in ("Failed", "Succeeded"):
            completed = True

        if transcription.status == "Succeeded":
            if properties.destination_container_url is not None:
                logging.info(
                    "Transcription succeeded. Results are located in your Azure Blob Storage."
                )
                break

            pag_files = api.transcriptions_list_files(transcription_id)
            for file_data in _paginate(api, pag_files):
                if file_data.kind != "Transcription":
                    continue

                audiofilename = file_data.name
                results_url = file_data.links.content_url
                results = requests.get(results_url)

                # Connect to the PostgreSQL database
                conn = psycopg2.connect(connection_string)
                cursor = conn.cursor()

                transcript_json = json.loads(results.content.decode("utf-8"))
                transcripttext = transcript_json["combinedRecognizedPhrases"][0][
                    "lexical"
                ]

                open("transcript_json.txt", "w").write(json.dumps(transcript_json))

                # Get list of questions generated from openai and extract the list
                # for inserting into the database
                questions = create_questions_list(config, title, transcripttext)
                list_of_questions = json.dumps(questions.choices[0].message.content)

                open("questions_json.txt", "w").write(list_of_questions)

                logging.info(list_of_questions)

                cursor.execute(
                    update_query,
                    (
                        list_of_questions,
                        results.content.decode("utf-8"),
                        transcripttext,
                        podcastid,
                        episodeid,
                    ),
                )

                # Commit the transaction
                conn.commit()

                # Close the cursor and the connection
                cursor.close()
                conn.close()

                logging.info(
                    f"Results for {audiofilename}:\n{results.content.decode('utf-8')}"
                )
        elif transcription.status == "Failed":
            logging.info(
                f"Transcription failed: {transcription.properties.error.message}"
            )


# accept the episode ID, transcribe, create question list and update in DB
if __name__ == "__main__":

    # accept from command line as input, the episodeid to transcribe
    episodeid = sys.argv[1]

    # Call the function to select the episode from the database
    result = select_from_episodes_with_episodeid(episodeid)
    audio_url = result[EpisodeAttributes.URL.value]
    title = result[EpisodeAttributes.TITLE.value]
    # transcribe_episode_from_audio_url(audio_url)

    # NOTE: podcastid is hardcoded to 1 as there is only a single one for now
    transcribe(1, episodeid, title, audio_url)
